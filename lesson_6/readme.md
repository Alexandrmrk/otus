1. Построил шардированный кластер из 3 кластерных нод(по 3 инстанса с репликацией) и с кластером конфига(3 инстанса) по аналогии с исходным файлом, который был приложен к уроку(добавил третий кластерный узел).

2. Добавил балансировку, нагрузил данными, выбрал хороший ключ шардирования(_id).

Для db bank на RS3 в начале шардирования было больше всего чанков. Спустя время, количество чанков стало примерно одинаковым(10 на RS1, 10 на RS2, 11 на RS3).

Для db config на RS1 в начале шардирования было больше всего чанков. Спустя время, количество чанков стало примерно одинаковым(342 на RS1, 342 на RS2, 341 на RS3).

4. Уронил db1 в RS1, который был PRIMARY. db2 в RS1, который был SECONDARY стал PRIMARY.

Проблемы:
Была проблема с доступом на удаленный сервер. Видимо по умолчанию используется своя зона, поэтому формировался неправильный путь.

В лекциях не раз упомяналось что в д.з понадобится команда mongod --bind_ip_all. Не понадобилась и долго думал, почему столько раз был на этом акцент.

Удалось создать роль в bd1 для RS1, но не удалось запустить сервер с аутентификацией.

Вопросы:
Не совсем понятно что такое db config(видимо это как то связано с config servers) и почему количество чанков в db bank в начале деления максимальное у RS3, а в db config в RS1?

Как отследить, что одна из нод на реплике упала?
Можно ли настроить автоматическое поднятие как в k8s?

В презентации было упомянуто, что "Важно заметить, что при достижении каким-либо чанком неделимого диапазона,
например (44, 45], деление происходить не будет и чанк будет расти свыше chunksize." Почему (44, 45] нельзя поделить? Тут скорее всего вопрос но более точное понимание.

Уронил RS1. Ввел команду sh.status() в mongos. Визуально ничего не изменилось. Как отследить подобные ошибки, если они возникнут по тем или иным причинам?
